{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab: Carbon Capture Simulations with VAEs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem Statement\n",
    "In efforts to reduce carbon emissions and capture carbon dioxide from the environment, analyzing vast datasets with advanced machine learning techniques can be pivotal. However, due to the complexity of carbon capture data, interpreting large-scale tabular data can be challenging. Variational Autoencoders (VAEs) offer a way to model complex, high-dimensional data distributions. This lab involves creating a VAE model for analyzing and generating synthetic data based on carbon capture datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Objective\n",
    "The objective of this lab is to:\n",
    "- Build and understand a Variational Autoencoder (VAE) architecture in TensorFlow, tailored for tabular data.\n",
    "- Apply the VAE to a carbon capture dataset to capture key data patterns.\n",
    "- Train the VAE model and evaluate its ability to generate new synthetic data samples.\n",
    "- Learn the principles of reconstruction and latent space regularization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### System Requirements\n",
    "\n",
    "**To Train the model in Local System**\n",
    "\n",
    "**Hardware:**\n",
    "- GPU (Recommended for faster training, e.g., Nvidia CUDA-compatible GPUs)\n",
    "- RAM: 8 GB or more\n",
    "- Storage: At least 5 GB of free space for datasets and model storage\n",
    "\n",
    "**Software:**\n",
    "- Python 3.x\n",
    "- TensorFlow 2.x\n",
    "- Pandas, NumPy\n",
    "- Matplotlib (Optional for visualization)\n",
    "- Jupyter Notebook or any Python IDE\n",
    "- GPU drivers and CUDA (if available)\n",
    "\n",
    "It is recommended to use google colab, so there will not be any need of setting up the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset\n",
    "The dataset used in this project is called carbon_capture_data.csv.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Libraries** <br>\n",
    "The code begins by importing essential libraries required for VAE model creation and data handling.<br>\n",
    "**tensorflow** is used to define and train the VAE model.<br>\n",
    "**numpy** assists in numerical data operations.<br>\n",
    "**pandas** is used for handling the tabular data format.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GPU Configuration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell of the code is useful if you have a GPU like Nvidia or If you are using a cloud platform like kaggle\n",
    "# Ensure GPU is being used\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    print(\"Using GPU\")\n",
    "else:\n",
    "    print(\"No GPU found, using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Dataset**<br>\n",
    "Load the carbon capture data. Here, data is read from a CSV file, assumed to be formatted with relevant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"carbon_capture_dataset.csv\")\n",
    "data = df.values\n",
    "num_features = data.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the Encoder Model**<br>\n",
    "The Encoder is the first half of the VAE. It compresses the input into a latent representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.dense1 = layers.Dense(64, activation='relu')\n",
    "        self.dense2 = layers.Dense(32, activation='relu')\n",
    "        self.dense_mean = layers.Dense(latent_dim) # Mean of latent distribution\n",
    "        self.dense_log_var = layers.Dense(latent_dim) # Log variance of latent distribution\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.dense2(x)\n",
    "        mean = self.dense_mean(x)\n",
    "        log_var = self.dense_log_var(x)\n",
    "        return mean, log_var\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Layers:**<br>\n",
    "- Dense layers (dense1 and dense2) are applied to reduce the feature space gradually.\n",
    "- dense_mean and dense_log_var produce the mean and log variance of the latent space distribution, crucial for sampling new data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the Decoder Model**<br>\n",
    "The Decoder reconstructs data from the compressed latent representation produced by the Encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dense1 = layers.Dense(32, activation='relu')\n",
    "        self.dense2 = layers.Dense(64, activation='relu')\n",
    "        self.output_layer = layers.Dense(num_features) # Reconstruct original data dimensions\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.dense2(x)\n",
    "        return self.output_layer(x)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Layers:**<br>\n",
    "- Dense layers (dense1 and dense2) decode the latent representation gradually back into the original feature space.\n",
    "- output_layer matches the original data dimensions for reconstruction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the VAE Loss Function**<br>\n",
    "This custom loss function combines Reconstruction Loss (ensuring data similarity) and KL Divergence (latent space regularization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(original, reconstructed, mean, log_var):\n",
    "    # Mean Squared Error for tabular data reconstruction loss\n",
    "    reconstruction_loss = tf.reduce_mean(tf.keras.losses.mse(original, reconstructed))\n",
    "    # KL divergence for latent space regularization\n",
    "    kl_loss = -0.5 * tf.reduce_mean(1 + log_var - tf.square(mean) - tf.exp(log_var))\n",
    "    return reconstruction_loss + kl_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Reconstruction Loss**: Measures the difference between original and reconstructed data.\n",
    "- **KL Divergence**: Regularizes the latent space, encouraging Gaussian distribution around zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Initialization and Training Configuration**<br>\n",
    "Set the latent dimension and initialize the Encoder and Decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model setup\n",
    "latent_dim = 2\n",
    "encoder = Encoder(latent_dim)\n",
    "decoder = Decoder(latent_dim)\n",
    "optimizer = tf.keras.optimizers.Adam()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **latent_dim**: Sets the dimensionality of the compressed representation.\n",
    "- **optimizer**: Adam optimizer is chosen for efficient gradient-based training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the Training Step**<br>\n",
    "The training step function performs forward and backward passes and updates weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(data_batch):\n",
    "    with tf.GradientTape() as tape:\n",
    "        mean, log_var = encoder(data_batch)\n",
    "        epsilon = tf.random.normal(shape=tf.shape(mean))\n",
    "        z = mean + tf.exp(log_var * 0.5) * epsilon\n",
    "        reconstructed = decoder(z)\n",
    "        loss = vae_loss(data_batch, reconstructed, mean, log_var)\n",
    "    gradients = tape.gradient(loss, encoder.trainable_variables + decoder.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, encoder.trainable_variables + decoder.trainable_variables))\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Gradient Tape**: Tracks operations for automatic differentiation.\n",
    "- **Reparameterization Trick**: Adds random noise to ensure sampling is differentiable.\n",
    "- **Loss Calculation**: Combines reconstruction and KL losses to guide training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Loop**<br>\n",
    "Iterate over epochs and batches, calculating the loss for each batch and updating the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 32\n",
    "dataset = tf.data.Dataset.from_tensor_slices(data).batch(batch_size)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch: {epoch+1}/{epochs}\")\n",
    "    for data_batch in dataset:\n",
    "        loss = train_step(data_batch)\n",
    "    print(f\"Loss: {loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Epochs**: Number of complete passes through the dataset.\n",
    "- **Batch Size**: Number of samples per batch to optimize model training speed and stability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate New Samples**<br>\n",
    "Generate synthetic data samples from random latent vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 10\n",
    "random_latent_vectors = tf.random.normal(shape=(num_samples, latent_dim))\n",
    "generated_samples = decoder(random_latent_vectors)\n",
    "\n",
    "print(\"Generated samples:\")\n",
    "print(generated_samples.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generated Samples: Random latent vectors sampled from a Gaussian are fed into the Decoder to create new synthetic data, representing simulated carbon capture data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
