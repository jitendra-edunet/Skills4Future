{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem Statement\n",
    "As the world moves towards more sustainable development, there is a growing need for energy-efficient architectural designs. Traditional methods of designing energy-efficient buildings are time-consuming and heavily reliant on human expertise. This project uses Generative Adversarial Networks (GANs) to generate synthetic architectural designs that simulate energy-efficient buildings. By training a GAN on existing architectural data, the model can generate novel, energy-efficient designs that could inspire architects and engineers in real-world projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Objective\n",
    "- To simulate energy-efficient architecture designs using GANs.\n",
    "- To train a GAN using existing energy-efficiency data to create new building designs.\n",
    "- To apply machine learning (specifically GANs) to a real-world problem of energy-efficient design."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### System Requirements\n",
    "\n",
    "**To Train the model in Local System**\n",
    "\n",
    "**Hardware:**\n",
    "- GPU (Recommended for faster training, e.g., Nvidia CUDA-compatible GPUs)\n",
    "- RAM: 8 GB or more\n",
    "- Storage: At least 5 GB of free space for datasets and model storage\n",
    "\n",
    "**Software:**\n",
    "- Python 3.x\n",
    "- TensorFlow 2.x\n",
    "- Pandas, NumPy\n",
    "- Matplotlib (Optional for visualization)\n",
    "- Jupyter Notebook or any Python IDE\n",
    "- GPU drivers and CUDA (if available)\n",
    "\n",
    "It is recommended to use google colab, so there will not be any need of setting up the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset\n",
    "The dataset used in this project is called energy_architecture_data.csv. This dataset contains features of energy-efficient architecture designs and their corresponding energy efficiency values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pandas is used for data manipulation.\n",
    "- NumPy is used for numerical operations.\n",
    "- TensorFlow and Keras are used for building and training the GAN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU Configuration\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "if len(physical_devices) > 0:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    print(\"Using GPU\")\n",
    "else:\n",
    "    print(\"No GPU found, using CPU\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part checks whether a GPU is available. If it is, it enables GPU acceleration for training; otherwise, it defaults to using the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Dataset\n",
    "data = pd.read_csv('energy_architecture_data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is loaded using Pandas. data.head() will show the first few rows of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the GAN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensions\n",
    "input_dim = 100  # Random noise vector input for the generator\n",
    "output_dim = data.shape[1]  # Number of features in the data\n",
    "\n",
    "# Generator Model\n",
    "def build_generator():\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Dense(128, activation=\"relu\", input_dim=input_dim),\n",
    "        layers.Dense(256, activation=\"relu\"),\n",
    "        layers.Dense(512, activation=\"relu\"),\n",
    "        layers.Dense(output_dim, activation=\"tanh\")  # Output layer\n",
    "    ])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Generator network creates synthetic samples from random noise. It consists of fully connected layers with increasing neurons and uses ReLU activation for hidden layers and Tanh activation for the output layer (to match the range of the data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator Model\n",
    "def build_discriminator():\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Dense(512, activation=\"relu\", input_shape=(output_dim,)),\n",
    "        layers.Dense(256, activation=\"relu\"),\n",
    "        layers.Dense(128, activation=\"relu\"),\n",
    "        layers.Dense(1, activation=\"sigmoid\")  # Binary classification\n",
    "    ])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Discriminator network is a binary classifier that distinguishes between real and fake samples. It uses ReLU activation for hidden layers and Sigmoid activation for the output layer (to output probabilities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating and Compiling Models\n",
    "generator = build_generator()\n",
    "discriminator = build_discriminator()\n",
    "\n",
    "discriminator.compile(optimizer=tf.keras.optimizers.Adam(0.0002, 0.5), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The generator and discriminator models are instantiated.\n",
    "- The discriminator is compiled with the Adam optimizer and binary crossentropy loss, as it performs binary classification (real vs. fake)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the GAN Model\n",
    "discriminator.trainable = False  # Freeze the discriminator's weights when training the GAN\n",
    "gan_input = layers.Input(shape=(input_dim,))\n",
    "fake_data = generator(gan_input)\n",
    "gan_output = discriminator(fake_data)\n",
    "gan = tf.keras.Model(gan_input, gan_output)\n",
    "gan.compile(optimizer=tf.keras.optimizers.Adam(0.0002, 0.5), loss=\"binary_crossentropy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The GAN model is constructed by connecting the generator and the discriminator.\n",
    "- The discriminator's weights are frozen during GAN training (it is only updated when training the discriminator)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the GAN Model\n",
    "def train(data, epochs, batch_size):\n",
    "    half_batch = int(batch_size / 2)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # --------------------- Train Discriminator ---------------------\n",
    "        # Set discriminator trainable to True\n",
    "        discriminator.trainable = True\n",
    "\n",
    "        # Select a random half-batch of real samples\n",
    "        idx = np.random.randint(0, data.shape[0], half_batch)\n",
    "        real_data = data.iloc[idx].values\n",
    "\n",
    "        # Generate a half-batch of fake samples\n",
    "        noise = np.random.normal(0, 1, (half_batch, input_dim))\n",
    "        fake_data = generator.predict(noise)\n",
    "\n",
    "        # Train the discriminator on real and fake samples\n",
    "        d_loss_real = discriminator.train_on_batch(real_data, np.ones((half_batch, 1)))\n",
    "        d_loss_fake = discriminator.train_on_batch(fake_data, np.zeros((half_batch, 1)))\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "        # --------------------- Train Generator ---------------------\n",
    "        # Set discriminator trainable to False for GAN training\n",
    "        discriminator.trainable = False\n",
    "\n",
    "        # Generate a batch of noise\n",
    "        noise = np.random.normal(0, 1, (batch_size, input_dim))\n",
    "        \n",
    "        # Train the generator \n",
    "        g_loss = gan.train_on_batch(noise, np.ones((batch_size, 1)))\n",
    "\n",
    "        # Print progress\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"epoch: {epoch+1}/{epochs} [D loss: {d_loss[0]:.4f}, acc.: {100*d_loss[1]:.2f}] [G loss: {g_loss}]\")\n",
    "        else:\n",
    "            print(f\"epoch: {epoch+1}/{epochs} ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This function trains the GAN for a set number of epochs.\n",
    "- In each epoch, the discriminator is trained with both real and fake data, and the generator is trained to fool the discriminator into thinking fake data is real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "train(data, epochs=5000, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the Model\n",
    "generator.save('generator_model.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, the generator model is saved to a file (generator_model.h5) for later use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function generate synthetic data\n",
    "def generate_synthetic_data(generator, num_samples=1):\n",
    "    noise_dim=100\n",
    "    # Generate random noise\n",
    "    noise = np.random.normal(0, 1, (num_samples, noise_dim))\n",
    "    \n",
    "    # Generate synthetic data using the generator\n",
    "    synthetic_data = generator.predict(noise)\n",
    "    return synthetic_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function generates synthetic architectural designs using the trained generator model. The generator is given random noise as input and produces synthetic data as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating 10 synthetic samples\n",
    "generate_synthetic_data(generator, num_samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Saved Model\n",
    "from tensorflow.keras.models import load_model\n",
    "loaded_generator = load_model('generator_model.h5')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
